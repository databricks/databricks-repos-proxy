{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09428fe1-c76a-4d03-8deb-acbd94050594",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Enable Git Proxy for private Git server connectivity in Repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d139a95c-8953-4a5a-8373-38e0d52b084a",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Overview\n",
    "This private preview feature is available on AWS and Azure.\n",
    "\n",
    "**Note**: an *admin* must run this notebook to enable the feature.\n",
    "\n",
    "\"Run all\" this notebook to set up a cluster that proxies requests to your private Git server. Running this notebook does the following things:\n",
    "\n",
    "0. Writes a shell script to DBFS (`dbfs:/databricks/db_repos_proxy/db_repos_proxy_init.sh`) that is used as a [cluster-scoped init script](https://docs.databricks.com/clusters/init-scripts.html#example-cluster-scoped-init-scripts).\n",
    "0. Creates a [single node cluster](https://docs.databricks.com/clusters/single-node.html) named `dp_git_proxy` that runs the init script on start-up. **Important**: all users in the workspace will be granted \"attach to\" permissions to the cluster.\n",
    "0. Enables a feature flag that controls whether Git requests in Repos are proxied via the cluster.\n",
    "\n",
    "You may need to wait several minutes after running this notebook for the cluster to reach a \"RUNNING\" state. It can also take up to 30 minutes for the feature flag configuration to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c91a47-13ed-4cf1-ad77-c6d6395fcff2",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Write Cluster Init Script to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527cc050-3110-4242-8bd9-75563714d39d",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_repos_proxy_init = \"\"\"\n",
    "#!/bin/bash\n",
    "set -x\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Install Python\n",
    "mkdir /databricks/db_repos_proxy\n",
    "cat >  /databricks/db_repos_proxy/db_repos_proxy.py <<EOF\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "from distutils.util import strtobool\n",
    "from http.server import HTTPServer, BaseHTTPRequestHandler\n",
    "from pathlib import Path\n",
    "\n",
    "import urllib3\n",
    "from urllib3 import PoolManager\n",
    "from urllib3.request import RequestMethods\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class ProxyConfig:\n",
    "    VERSION = \"0.0.20\"\n",
    "    PORT = os.environ.get(\"PROXY_PORT\", 8000)\n",
    "    LOG_FILE_PATH = os.environ.get(\n",
    "        \"LOG_FILE_PATH\", tempfile.NamedTemporaryFile(delete=False).name\n",
    "    )\n",
    "    ENABLE_SSL_VERIFICATION = bool(\n",
    "        strtobool(os.environ.get(\"ENABLE_SSL_VERIFICATION\", \"True\"))\n",
    "    )\n",
    "    ENABLE_LOGGING = bool(strtobool(os.environ.get(\"ENABLE_DB_REPOS_PROXY\", \"False\")))\n",
    "    CA_CERT_PATH = os.environ.get(\"CA_CERT_PATH\", \"\")\n",
    "\n",
    "\n",
    "# Make LOG_FILE_PATH directory if it doesn't exist\n",
    "Path(ProxyConfig.LOG_FILE_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    handlers=[\n",
    "        logging.FileHandler(ProxyConfig.LOG_FILE_PATH),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Internal Utils\n",
    "def log_message(message):\n",
    "    logger.info(f\"Version: {ProxyConfig.VERSION} {message}\")\n",
    "\n",
    "\n",
    "def log_headers(message: str = \"\", headers: dict = {}):\n",
    "    header_strs = []\n",
    "    for k, v in headers.items():\n",
    "        if \"authorization\" not in k.lower():\n",
    "            header_strs.append(f\"{k}: {v}\")\n",
    "        else:\n",
    "            header_strs.append(f\"{k}: <REDACTED>\")\n",
    "    header_str = \" \".join(header_strs)\n",
    "    logger.info(\n",
    "        f\"Version: {ProxyConfig.VERSION} Message: {message} Headers: {header_str}\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Proxy Utils\n",
    "def set_headers(headers: dict, handler: BaseHTTPRequestHandler):\n",
    "    for k, v in headers.items():\n",
    "        handler.send_header(k, v)\n",
    "    handler.end_headers()\n",
    "\n",
    "\n",
    "def set_status_code(code: int, handler: BaseHTTPRequestHandler):\n",
    "    handler.send_response(code)\n",
    "\n",
    "\n",
    "def set_json_body(data: dict, handler: BaseHTTPRequestHandler):\n",
    "    handler.wfile.write(json.dumps(data).encode())\n",
    "\n",
    "\n",
    "def set_binary_body(data: bin, handler: BaseHTTPRequestHandler):\n",
    "    handler.wfile.write(data)\n",
    "\n",
    "\n",
    "# Databricks Header Logic\n",
    "\n",
    "def disable_chunking_headers(headers):\n",
    "    new_headers = {}\n",
    "    for k, v in headers.items():\n",
    "        if k.lower() != \"transfer-encoding\":\n",
    "            if k.lower() == \"connection\":\n",
    "                new_headers[k] = \"close\"\n",
    "            else:\n",
    "                new_headers[k] = v\n",
    "    return new_headers\n",
    "\n",
    "\n",
    "def copy_request_headers(headers, allowed_list=[]):\n",
    "    lowercase_allowed_list = [x.lower() for x in allowed_list]\n",
    "    copied_headers = {}\n",
    "    for header_key, header_value in headers.items():\n",
    "        if header_key.lower() in lowercase_allowed_list:\n",
    "            copied_headers[header_key] = header_value\n",
    "    return disable_chunking_headers(copied_headers)\n",
    "\n",
    "\n",
    "def copy_response_headers(headers):\n",
    "    copied_headers = {}\n",
    "    for header_key, header_value in headers.items():\n",
    "        copied_headers[header_key] = header_value\n",
    "    return disable_chunking_headers(copied_headers)\n",
    "\n",
    "\n",
    "DB_VERSION_HEADER_KEY = \"X-Databricks-Proxy-Server-Version\"\n",
    "DB_FORWARD_HEADER_PREFIX = \"x-databricks-forward-header-\"\n",
    "DB_FORWARD_HEADER_KEYS = \"x-databricks-allowed-headers\"\n",
    "\n",
    "DELIMITER = \",\"\n",
    "\n",
    "\n",
    "def copy_header_proxy_for_destination(headers):\n",
    "    allowed_header_keys = []\n",
    "    if DB_FORWARD_HEADER_KEYS in headers:\n",
    "        allowed_header_value = headers[DB_FORWARD_HEADER_KEYS]\n",
    "        for allowed_header_key in allowed_header_value.split(DELIMITER):\n",
    "            allowed_header_keys.append(allowed_header_key)\n",
    "    response_headers = copy_request_headers(headers, allowed_list=allowed_header_keys)\n",
    "\n",
    "    new_headers = {}\n",
    "    for header_key in headers.keys():\n",
    "        if header_key.lower().startswith(DB_FORWARD_HEADER_PREFIX):\n",
    "            new_header_key = header_key[len(DB_FORWARD_HEADER_PREFIX):]\n",
    "            if len(new_header_key):\n",
    "                new_headers[new_header_key] = headers[header_key]\n",
    "\n",
    "    # Forward headers always supersede strip headers\n",
    "    response_headers.update(new_headers)\n",
    "\n",
    "    for org_key in headers.keys():\n",
    "        if org_key not in response_headers:\n",
    "            log_message(\"Stripped header: {}\".format(org_key))\n",
    "    return response_headers\n",
    "\n",
    "\n",
    "# Forward response from destination to control plane\n",
    "def forward_proxy_response(handler: BaseHTTPRequestHandler, response: RequestMethods):\n",
    "    set_status_code(code=response.status, handler=handler)\n",
    "    response_headers = copy_response_headers(response.headers)\n",
    "    response_headers[DB_VERSION_HEADER_KEY] = ProxyConfig.VERSION\n",
    "    set_headers(headers=response_headers, handler=handler)\n",
    "    log_headers(message=\"Forward headers\", headers=handler.headers)\n",
    "    set_binary_body(data=response.data, handler=handler)\n",
    "\n",
    "\n",
    "# Handlers\n",
    "\n",
    "HEALTH_PATH = \"/databricks/health\"\n",
    "\n",
    "\n",
    "def do_health(handler: BaseHTTPRequestHandler):\n",
    "    set_status_code(200, handler)\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "    }\n",
    "    set_headers(headers=headers, handler=handler)\n",
    "    data = {\n",
    "        \"status\": \"ok\",\n",
    "        \"version\": ProxyConfig.VERSION,\n",
    "    }\n",
    "    set_json_body(data=data, handler=handler)\n",
    "\n",
    "\n",
    "def get_path_to_handler():\n",
    "    path_to_handler = {\n",
    "        HEALTH_PATH: do_health,\n",
    "    }\n",
    "    return path_to_handler\n",
    "\n",
    "\n",
    "def get_route_handler(path: str, path_to_handler: dict):\n",
    "    for route, handler in path_to_handler.items():\n",
    "        if path.startswith(route):\n",
    "            return handler\n",
    "    return None\n",
    "\n",
    "\n",
    "def do_proxy_get(handler: BaseHTTPRequestHandler, pool_manager: PoolManager):\n",
    "    request_headers = copy_header_proxy_for_destination(headers=handler.headers)\n",
    "    # Proxy to Destination\n",
    "    log_headers(\n",
    "        message=f\"Outgoing GET {handler.destination_url} headers\",\n",
    "        headers=request_headers,\n",
    "    )\n",
    "    response = pool_manager.request(\n",
    "        method=\"GET\",\n",
    "        url=handler.destination_url,\n",
    "        headers=request_headers,\n",
    "        decode_content=False,\n",
    "    )\n",
    "\n",
    "    # Proxy to Control Plane\n",
    "    forward_proxy_response(handler=handler, response=response)\n",
    "\n",
    "\n",
    "def do_proxy_post(handler: BaseHTTPRequestHandler, pool_manager: PoolManager):\n",
    "    content_len = int(handler.headers.get(\"Content-Length\", 0))\n",
    "    post_body = handler.rfile.read(content_len)\n",
    "    request_headers = copy_header_proxy_for_destination(headers=handler.headers)\n",
    "\n",
    "    log_headers(\n",
    "        message=f\"Outgoing POST {handler.destination_url} headers\",\n",
    "        headers=request_headers,\n",
    "    )\n",
    "    response = pool_manager.request(\n",
    "        method=\"POST\",\n",
    "        url=handler.destination_url,\n",
    "        headers=request_headers,\n",
    "        body=post_body,\n",
    "        decode_content=False,\n",
    "    )\n",
    "\n",
    "    # Proxy to Control Plane\n",
    "    forward_proxy_response(handler=handler, response=response)\n",
    "\n",
    "\n",
    "def get_pool_manager():\n",
    "    _pool_manager_config = {}\n",
    "    if ProxyConfig.ENABLE_SSL_VERIFICATION is False:\n",
    "        _pool_manager_config[\"cert_reqs\"] = \"CERT_NONE\"\n",
    "    elif ProxyConfig.CA_CERT_PATH:\n",
    "        _pool_manager_config[\"ca_certs\"] = ProxyConfig.CA_CERT_PATH\n",
    "    return urllib3.PoolManager(**_pool_manager_config)\n",
    "\n",
    "\n",
    "http = get_pool_manager()\n",
    "\n",
    "\n",
    "class ProxyRequestHandler(BaseHTTPRequestHandler):\n",
    "    @property\n",
    "    def destination_url(self):\n",
    "        return \"https:/{}\".format(self.path)\n",
    "\n",
    "    def do_GET(self):\n",
    "        log_message(f\"do_GET: {self.destination_url}\")\n",
    "        log_headers(message=\"Incoming GET headers\", headers=self.headers)\n",
    "        matched_handler = get_route_handler(\n",
    "            path=self.path,\n",
    "            path_to_handler=get_path_to_handler(),\n",
    "        )\n",
    "        if matched_handler:\n",
    "            return matched_handler(self)\n",
    "        do_proxy_get(handler=self, pool_manager=http)\n",
    "\n",
    "    def do_POST(self):\n",
    "        log_message(f\"do_POST: {self.destination_url}\")\n",
    "        log_headers(message=\"Incoming POST headers\", headers=self.headers)\n",
    "        do_proxy_post(handler=self, pool_manager=http)\n",
    "\n",
    "    def do_HEAD(self):\n",
    "        log_message(f\"do_HEAD: {self.destination_url}\")\n",
    "        raise Exception(f\"HEAD not supported {self.destination_url}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    server_address = (\"\", ProxyConfig.PORT)\n",
    "    log_message(\n",
    "        f\"Data-plane proxy server version {ProxyConfig.VERSION} binding to {server_address} ...\"\n",
    "    )\n",
    "    log_message(f\"ProxyConfig {ProxyConfig.__dict__}\")\n",
    "    httpd = HTTPServer(server_address, ProxyRequestHandler)\n",
    "    httpd.serve_forever()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "EOF\n",
    "#--------------------------------------------------\n",
    "# Setup Systemd\n",
    "cat > /etc/systemd/system/db_repos_proxy.service <<EOF\n",
    "[Service]\n",
    "Type=simple\n",
    "Environment=ENABLE_SSL_VERIFICATION=true CA_CERT_PATH=''\n",
    "ExecStart=/databricks/python3/bin/python3 -u /databricks/db_repos_proxy/db_repos_proxy.py\n",
    "StandardInput=null\n",
    "StandardOutput=file:/databricks/db_repos_proxy/daemon.log\n",
    "StandardError=file:/databricks/db_repos_proxy/daemon.log\n",
    "Restart=always\n",
    "RestartSec=1\n",
    "\n",
    "[Unit]\n",
    "Description=Git Proxy Service\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "EOF\n",
    "#--------------------------------------------------\n",
    "\n",
    "systemctl daemon-reload\n",
    "systemctl enable db_repos_proxy.service\n",
    "systemctl start db_repos_proxy.service\n",
    "\"\"\"  # db_repos_proxy_init_end\n",
    "\n",
    "location = \"/databricks/db_repos_proxy/db_repos_proxy_init.sh\"\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/db_repos_proxy/\")\n",
    "dbutils.fs.put(location, db_repos_proxy_init, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e1623df-034f-4853-b544-cf70a491702d",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the proxy cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a40944-3ecb-4b7b-8e3a-9e638e5d37eb",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"cluster-name\", \"\", \"Git Proxy Cluster Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ae215e-ea5a-4e45-b31a-e6bd0effbc85",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "admin_token = (\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    ")\n",
    "databricks_instance = (\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    ")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {admin_token}\"}\n",
    "\n",
    "# Clusters\n",
    "CLUSTERS_LIST_ENDPOINT = \"/api/2.0/clusters/list\"\n",
    "CLUSTERS_CREATE_ENDPOINT = \"/api/2.0/clusters/create\"\n",
    "CLUSTERS_LIST_NODE_TYPES_ENDPOINT = \"/api/2.0/clusters/list-node-types\"\n",
    "\n",
    "# Permissions\n",
    "UPDATE_PERMISSIONS_ENDPOINT = \"/api/2.0/permissions/clusters\"\n",
    "\n",
    "# Workspace Conf\n",
    "WORKSPACE_CONF_ENDPOINT = \"/api/2.0/workspace-conf\"\n",
    "\n",
    "# get name to use for cluster\n",
    "cluster_name = \"dp_git_proxy\"  # default name\n",
    "widget_value = dbutils.widgets.get(\"cluster-name\")\n",
    "workspace_conf_value = requests.get(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=gitProxyClusterName\",\n",
    "    headers=headers,\n",
    ").json()[\"gitProxyClusterName\"]\n",
    "print(f\"widget value: {widget_value}\")\n",
    "print(f\"workspace conf value: {workspace_conf_value}\")\n",
    "\n",
    "if widget_value:\n",
    "    cluster_name = widget_value\n",
    "elif workspace_conf_value:\n",
    "    cluster_name = workspace_conf_value\n",
    "print(f\"Using cluster name {cluster_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af9eb9d-f298-465f-83f3-7464fbbbd184",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "create_cluster_data = {\n",
    "    \"cluster_name\": cluster_name,\n",
    "    \"spark_version\": \"12.2.x-scala2.12\",\n",
    "    \"num_workers\": 0,\n",
    "    \"autotermination_minutes\": 0,\n",
    "    \"spark_conf\": {\n",
    "        \"spark.databricks.cluster.profile\": \"singleNode\",\n",
    "        \"spark.master\": \"local[*]\",\n",
    "    },\n",
    "    \"custom_tags\": {\"ResourceClass\": \"SingleNode\"},\n",
    "    \"init_scripts\": {\n",
    "        \"dbfs\": {\n",
    "            \"destination\": \"dbfs:/databricks/db_repos_proxy/db_repos_proxy_init.sh\"\n",
    "        }\n",
    "    },\n",
    "}\n",
    "# get list of node types to determine whether this workspace is on AWS or Azure\n",
    "clusters_node_types = requests.get(\n",
    "    databricks_instance + CLUSTERS_LIST_NODE_TYPES_ENDPOINT, headers=headers\n",
    ").json()[\"node_types\"]\n",
    "node_type_ids = [type[\"node_type_id\"] for type in clusters_node_types]\n",
    "aws_node_type_id = \"m5.large\"\n",
    "azure_node_type_id = \"Standard_DS3_v2\"\n",
    "if aws_node_type_id in node_type_ids:\n",
    "    create_cluster_data = {\n",
    "        **create_cluster_data,\n",
    "        \"node_type_id\": aws_node_type_id,\n",
    "        \"aws_attributes\": {\"ebs_volume_count\": \"1\", \"ebs_volume_size\": \"32\"},\n",
    "    }\n",
    "elif azure_node_type_id in node_type_ids:\n",
    "    create_cluster_data = {**create_cluster_data, \"node_type_id\": azure_node_type_id}\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Node types {aws_node_type_id} or {azure_node_type_id} do not exist. Make sure you are on AWS or Azure, or contact support.\"\n",
    "    )\n",
    "\n",
    "# Note: this only returns up to 100 terminated all-purpose clusters in the past 30 days\n",
    "clusters_list_response = requests.get(\n",
    "    databricks_instance + CLUSTERS_LIST_ENDPOINT, headers=headers\n",
    ").json()\n",
    "clusters_list = clusters_list_response[\"clusters\"]\n",
    "clusters_names = [\n",
    "    cluster[\"cluster_name\"] for cluster in clusters_list_response[\"clusters\"]\n",
    "]\n",
    "print(f\"List of existing clusters: {clusters_names}\")\n",
    "\n",
    "if cluster_name in clusters_names:\n",
    "    raise ValueError(\n",
    "        f\"Cluster called {cluster_name} already exists. Please delete this cluster and re-run this notebook\"\n",
    "    )\n",
    "else:\n",
    "    # Create a new cluster named cluster_name that will proxy requests to the private Git server\n",
    "    print(f\"Create cluster POST request data: {create_cluster_data}\")\n",
    "    clusters_create_response = requests.post(\n",
    "        databricks_instance + CLUSTERS_CREATE_ENDPOINT,\n",
    "        headers=headers,\n",
    "        json=create_cluster_data,\n",
    "    ).json()\n",
    "    print(f\"Create cluster response: {clusters_create_response}\")\n",
    "    cluster_id = clusters_create_response[\"cluster_id\"]\n",
    "    print(f\"Created new cluster with id {cluster_id}\")\n",
    "    update_permissions_data = {\n",
    "        \"access_control_list\": [\n",
    "            {\"group_name\": \"users\", \"permission_level\": \"CAN_ATTACH_TO\"}\n",
    "        ]\n",
    "    }\n",
    "    update_permissions_response = requests.patch(\n",
    "        databricks_instance + UPDATE_PERMISSIONS_ENDPOINT + f\"/{cluster_id}\",\n",
    "        headers=headers,\n",
    "        json=update_permissions_data,\n",
    "    ).json()\n",
    "    print(f\"Update permissions response: {update_permissions_response}\")\n",
    "    print(f\"Gave all users ATTACH TO permissions to cluster {cluster_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cec1268f-5134-4bf5-9fc0-23140492558b",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Flip the feature flag!\n",
    "This flips the feature flag to route Git requests to the cluster. The change should take into effect within an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0829abfd-638c-4db1-9da4-d2c9e219f53f",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "patch_enable_git_proxy_data = {\"enableGitProxy\": \"true\"}\n",
    "patch_git_proxy_cluster_name_data = {\"gitProxyClusterName\": cluster_name}\n",
    "requests.patch(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT,\n",
    "    headers=headers,\n",
    "    json=patch_enable_git_proxy_data,\n",
    ")\n",
    "requests.patch(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT,\n",
    "    headers=headers,\n",
    "    json=patch_git_proxy_cluster_name_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814082f8-5526-4835-b864-0a426c1b6926",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check that flag has been set\n",
    "If the command below returns with `{\"enableGitProxy\":\"true\"}`, you should be all set. Also, if you configured a custom cluster name using the widget, check that the cluster name in the response matches the name you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68066546-21ff-4523-a5dc-9760b225474a",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_flag_response = requests.get(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=enableGitProxy\",\n",
    "    headers=headers,\n",
    ").json()\n",
    "get_cluster_name_response = requests.get(\n",
    "    databricks_instance + WORKSPACE_CONF_ENDPOINT + \"?keys=gitProxyClusterName\",\n",
    "    headers=headers,\n",
    ").json()\n",
    "print(f\"Get enableGitProxy response: {get_flag_response}\")\n",
    "print(f\"Get gitProxyClusterName response: {get_cluster_name_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571a574b-f0f6-4646-8429-a92e5a6133eb",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validation steps\n",
    "Attach this notebook to the **Git proxy cluster** that was just created and follow the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613d12e5-9614-48cd-a928-61ce8c9c13ca",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "systemctl status db_repos_proxy.service\n",
    "journalctl -u db_repos_proxy.service\n",
    "cat /databricks/db_repos_proxy/daemon.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f827ab3-f507-4728-95aa-8b8bc46b4989",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e188b4a-dbd3-42d3-83c3-96a1c2e482a4",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "curl localhost:8000/databricks/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "120f1fcc-abc2-49d8-acd0-6eaaca0f9e9c",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1649133805813433,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "enable_git_proxy_source",
   "notebookOrigID": 1649133805813416,
   "widgets": {
    "cluster-name": {
     "currentValue": "single_proxy",
     "nuid": "f92f1a5b-b6e6-4cce-bb14-8a0a199fa3d0",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Git Proxy Cluster Name",
      "name": "cluster-name",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}